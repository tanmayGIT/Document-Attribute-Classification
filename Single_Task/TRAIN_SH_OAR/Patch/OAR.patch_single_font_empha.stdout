Hi, I am here dear available : 
{'resume': 'janinah', 'debug': False, 'model': 'resnet', 'taskname': 'font_emphasis', 'batch_size': 500, 'number_of_class': 4}
The cuda is available :  True
The number of classes are 4 and the batch size is 500
Training path exists
Validation path exists
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Train_Data_Patch/
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Validation_Data_Patch/
The size of train dataset: 7261574
The size of validation dataset: 1309966
(tensor([[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
         [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
         [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
         ...,
         [-1.5870, -1.7754,  0.2282,  ...,  2.2318,  2.2318,  2.2318],
         [-1.4158, -1.6727,  0.1083,  ...,  2.2318,  2.2318,  2.2318],
         [-0.1657, -0.6965,  0.6049,  ...,  2.2318,  2.2318,  2.2318]],

        [[ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
         [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
         [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
         ...,
         [-1.4930, -1.6856,  0.3627,  ...,  2.4111,  2.4111,  2.4111],
         [-1.3179, -1.5805,  0.2402,  ...,  2.4111,  2.4111,  2.4111],
         [-0.0399, -0.5826,  0.7479,  ...,  2.4111,  2.4111,  2.4111]],

        [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
         [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
         [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
         ...,
         [-1.2641, -1.4559,  0.5834,  ...,  2.6226,  2.6226,  2.6226],
         [-1.0898, -1.3513,  0.4614,  ...,  2.6226,  2.6226,  2.6226],
         [ 0.1825, -0.3578,  0.9668,  ...,  2.6226,  2.6226,  2.6226]]]), tensor([1, 0, 0]), tensor([1, 0, 0]), tensor([1, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
Let's use 2 GPUs!
model and cuda mixing done
I am inside right trainer and task name is :  font_emphasis
Epoch 0/89
----------
train total loss: 0.7687
train mean_loss of all the batches: 0.7687
train variable_Acc: 0.9746
val total loss: 0.7716
val mean_loss of all the batches: 0.7718
val variable_Acc: 0.9710
saving with loss of 0.7715591179676312 improved over previous 0.0
Saving Model...

Epoch 1/89
----------
train total loss: 0.7605
train mean_loss of all the batches: 0.7605
train variable_Acc: 0.9829
val total loss: 0.7677
val mean_loss of all the batches: 0.7680
val variable_Acc: 0.9749
saving with loss of 0.7677390190387517 improved over previous 0.7715591179676312
Saving Model...

Epoch 2/89
----------
train total loss: 0.7583
train mean_loss of all the batches: 0.7583
train variable_Acc: 0.9852
val total loss: 0.7650
val mean_loss of all the batches: 0.7653
val variable_Acc: 0.9777
saving with loss of 0.7650443739654542 improved over previous 0.7677390190387517
Saving Model...

Epoch 3/89
----------
train total loss: 0.7571
train mean_loss of all the batches: 0.7571
train variable_Acc: 0.9863
val total loss: 0.7660
val mean_loss of all the batches: 0.7663
val variable_Acc: 0.9766
saving with loss of 0.7660312820553589 improved over previous 0.7650443739654542
Saving Model...

Epoch 4/89
----------
train total loss: 0.7563
train mean_loss of all the batches: 0.7563
train variable_Acc: 0.9871
val total loss: 0.7633
val mean_loss of all the batches: 0.7636
val variable_Acc: 0.9794
saving with loss of 0.7633065199298807 improved over previous 0.7660312820553589
Saving Model...

Epoch 5/89
----------
train total loss: 0.7557
train mean_loss of all the batches: 0.7557
train variable_Acc: 0.9878
val total loss: 0.7626
val mean_loss of all the batches: 0.7629
val variable_Acc: 0.9801
saving with loss of 0.7626425687661443 improved over previous 0.7633065199298807
Saving Model...

Epoch 6/89
----------
train total loss: 0.7553
train mean_loss of all the batches: 0.7553
train variable_Acc: 0.9882
