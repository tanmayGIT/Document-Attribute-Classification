Hi, I am here dear available : 
{'resume': 'janinah', 'debug': False, 'model': 'resnet', 'taskname': 'font_emphasis', 'batch_size': 800, 'number_of_class': 4}
The cuda is available :  True
The number of classes are 4 and the batch size is 800
Training path exists
Validation path exists
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Train_Data_Server/
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Validation_Data_Server/
The size of train dataset: 8099561
The size of validation dataset: 1344016
(tensor([[[ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         ...,
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357]],

        [[ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         ...,
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405]],

        [[ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         ...,
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119]]]), tensor([1, 0, 0]), tensor([1, 0, 0]), tensor([1, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
Let's use 2 GPUs!
model and cuda mixing done
I am inside right trainer and task name is :  font_emphasis
Epoch 0/89
----------
train total loss: 0.8359
train mean_loss of all the batches: 0.8359
train variable_Acc: 0.9062
val total loss: 0.8526
val mean_loss of all the batches: 0.8527
val variable_Acc: 0.8890
saving with loss of 0.8526447676438391 improved over previous 0.0
Saving Model...

Epoch 1/89
----------
train total loss: 0.8109
train mean_loss of all the batches: 0.8110
train variable_Acc: 0.9316
val total loss: 0.8456
val mean_loss of all the batches: 0.8456
val variable_Acc: 0.8961
saving with loss of 0.8456218911912308 improved over previous 0.8526447676438391
Saving Model...

Epoch 2/89
----------
train total loss: 0.8031
train mean_loss of all the batches: 0.8031
train variable_Acc: 0.9395
val total loss: 0.8382
val mean_loss of all the batches: 0.8382
val variable_Acc: 0.9036
saving with loss of 0.8381892187740319 improved over previous 0.8456218911912308
Saving Model...

Epoch 3/89
----------
train total loss: 0.7985
train mean_loss of all the batches: 0.7985
train variable_Acc: 0.9442
val total loss: 0.8326
val mean_loss of all the batches: 0.8326
val variable_Acc: 0.9093
saving with loss of 0.8325922768169443 improved over previous 0.8381892187740319
Saving Model...

Epoch 4/89
----------
train total loss: 0.7952
train mean_loss of all the batches: 0.7953
train variable_Acc: 0.9475
val total loss: 0.8322
val mean_loss of all the batches: 0.8322
val variable_Acc: 0.9099
saving with loss of 0.8321767892441981 improved over previous 0.8325922768169443
Saving Model...

Epoch 5/89
----------
train total loss: 0.7928
train mean_loss of all the batches: 0.7928
train variable_Acc: 0.9500
val total loss: 0.8278
val mean_loss of all the batches: 0.8278
val variable_Acc: 0.9143
saving with loss of 0.8278275694223082 improved over previous 0.8321767892441981
Saving Model...

Epoch 6/89
----------
train total loss: 0.7907
train mean_loss of all the batches: 0.7907
train variable_Acc: 0.9521
val total loss: 0.8255
val mean_loss of all the batches: 0.8255
val variable_Acc: 0.9167
saving with loss of 0.8254553997846299 improved over previous 0.8278275694223082
Saving Model...

Epoch 7/89
----------
train total loss: 0.7892
train mean_loss of all the batches: 0.7892
train variable_Acc: 0.9537
val total loss: 0.8256
val mean_loss of all the batches: 0.8256
val variable_Acc: 0.9167
saving with loss of 0.8255520059492453 improved over previous 0.8254553997846299
Saving Model...

Epoch 8/89
----------
train total loss: 0.7878
train mean_loss of all the batches: 0.7878
train variable_Acc: 0.9551
val total loss: 0.8257
val mean_loss of all the batches: 0.8257
val variable_Acc: 0.9165
saving with loss of 0.8256909816776367 improved over previous 0.8255520059492453
Saving Model...

Epoch 9/89
----------
train total loss: 0.7801
train mean_loss of all the batches: 0.7801
train variable_Acc: 0.9630
val total loss: 0.8102
val mean_loss of all the batches: 0.8102
val variable_Acc: 0.9323
saving with loss of 0.8101694864967951 improved over previous 0.8256909816776367
Saving Model...

Epoch 10/89
----------
train total loss: 0.7785
train mean_loss of all the batches: 0.7785
train variable_Acc: 0.9647
val total loss: 0.8091
val mean_loss of all the batches: 0.8091
val variable_Acc: 0.9333
saving with loss of 0.8090745315181532 improved over previous 0.8101694864967951
Saving Model...

Epoch 11/89
----------
train total loss: 0.7779
train mean_loss of all the batches: 0.7779
train variable_Acc: 0.9653
val total loss: 0.8093
val mean_loss of all the batches: 0.8093
val variable_Acc: 0.9332
saving with loss of 0.8092528790162079 improved over previous 0.8090745315181532
Saving Model...

Epoch 12/89
----------
train total loss: 0.7774
train mean_loss of all the batches: 0.7775
train variable_Acc: 0.9657
val total loss: 0.8088
val mean_loss of all the batches: 0.8088
val variable_Acc: 0.9336
saving with loss of 0.8088241591569751 improved over previous 0.8092528790162079
Saving Model...

Epoch 13/89
----------
