Hi, I am here dear available : 
{'resume': 'janinah', 'debug': False, 'model': 'resnet', 'taskname': 'scanning', 'batch_size': 800, 'number_of_class': 3}
The cuda is available :  True
The number of classes are 3 and the batch size is 800
Training path exists
Validation path exists
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Train_Data_Server/
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Validation_Data_Server/
The size of train dataset: 8099561
The size of validation dataset: 1344016
(tensor([[[ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         ...,
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357]],

        [[ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         ...,
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405]],

        [[ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         ...,
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119]]]), tensor([1, 0, 0]), tensor([1, 0, 0]), tensor([1, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
Let's use 2 GPUs!
model and cuda mixing done
I am inside right trainer and task name is :  scanning
Epoch 0/89
----------
train total loss: 0.5698
train mean_loss of all the batches: 0.5698
train variable_Acc: 0.9812
val total loss: 0.5749
val mean_loss of all the batches: 0.5749
val variable_Acc: 0.9759
saving with loss of 0.5749289588507248 improved over previous 0.0
Saving Model...

Epoch 1/89
----------
train total loss: 0.5627
train mean_loss of all the batches: 0.5627
train variable_Acc: 0.9885
val total loss: 0.5694
val mean_loss of all the batches: 0.5694
val variable_Acc: 0.9816
saving with loss of 0.5694123188275197 improved over previous 0.5749289588507248
Saving Model...

Epoch 2/89
----------
train total loss: 0.5609
train mean_loss of all the batches: 0.5610
train variable_Acc: 0.9902
val total loss: 0.5714
val mean_loss of all the batches: 0.5714
val variable_Acc: 0.9796
saving with loss of 0.5713743593571841 improved over previous 0.5694123188275197
Saving Model...

Epoch 3/89
----------
train total loss: 0.5599
train mean_loss of all the batches: 0.5599
train variable_Acc: 0.9913
val total loss: 0.5672
val mean_loss of all the batches: 0.5672
val variable_Acc: 0.9839
saving with loss of 0.5671927831245064 improved over previous 0.5713743593571841
Saving Model...

Epoch 4/89
----------
train total loss: 0.5592
train mean_loss of all the batches: 0.5592
train variable_Acc: 0.9920
val total loss: 0.5665
val mean_loss of all the batches: 0.5665
val variable_Acc: 0.9846
saving with loss of 0.5664751841365419 improved over previous 0.5671927831245064
Saving Model...

Epoch 5/89
----------
train total loss: 0.5587
train mean_loss of all the batches: 0.5587
train variable_Acc: 0.9925
val total loss: 0.5642
val mean_loss of all the batches: 0.5642
val variable_Acc: 0.9870
saving with loss of 0.5641867781796328 improved over previous 0.5664751841365419
Saving Model...

Epoch 6/89
----------
train total loss: 0.5582
train mean_loss of all the batches: 0.5583
train variable_Acc: 0.9930
val total loss: 0.5647
val mean_loss of all the batches: 0.5647
val variable_Acc: 0.9864
saving with loss of 0.5647158480756611 improved over previous 0.5641867781796328
Saving Model...

Epoch 7/89
----------
train total loss: 0.5579
train mean_loss of all the batches: 0.5579
train variable_Acc: 0.9933
val total loss: 0.5635
val mean_loss of all the batches: 0.5636
val variable_Acc: 0.9876
saving with loss of 0.5635478151600835 improved over previous 0.5647158480756611
Saving Model...

Epoch 8/89
----------
train total loss: 0.5576
train mean_loss of all the batches: 0.5576
train variable_Acc: 0.9937
val total loss: 0.5637
val mean_loss of all the batches: 0.5637
val variable_Acc: 0.9874
saving with loss of 0.5636718836604517 improved over previous 0.5635478151600835
Saving Model...

Epoch 9/89
----------
train total loss: 0.5558
train mean_loss of all the batches: 0.5558
train variable_Acc: 0.9955
val total loss: 0.5608
val mean_loss of all the batches: 0.5608
val variable_Acc: 0.9904
saving with loss of 0.5607848138625057 improved over previous 0.5636718836604517
Saving Model...

Epoch 10/89
----------
train total loss: 0.5554
train mean_loss of all the batches: 0.5554
train variable_Acc: 0.9959
val total loss: 0.5602
val mean_loss of all the batches: 0.5603
val variable_Acc: 0.9910
saving with loss of 0.5602445543919987 improved over previous 0.5607848138625057
Saving Model...

Epoch 11/89
----------
train total loss: 0.5553
train mean_loss of all the batches: 0.5553
train variable_Acc: 0.9961
val total loss: 0.5604
val mean_loss of all the batches: 0.5604
val variable_Acc: 0.9908
saving with loss of 0.5604322333473306 improved over previous 0.5602445543919987
Saving Model...

Epoch 12/89
----------
train total loss: 0.5552
train mean_loss of all the batches: 0.5552
train variable_Acc: 0.9962
val total loss: 0.5602
val mean_loss of all the batches: 0.5602
val variable_Acc: 0.9910
saving with loss of 0.5601933944540503 improved over previous 0.5604322333473306
Saving Model...

Epoch 13/89
----------
train total loss: 0.5550
train mean_loss of all the batches: 0.5551
train variable_Acc: 0.9963
val total loss: 0.5602
val mean_loss of all the batches: 0.5602
val variable_Acc: 0.9910
saving with loss of 0.5602120656705813 improved over previous 0.5601933944540503
Saving Model...

Epoch 14/89
----------
train total loss: 0.5550
train mean_loss of all the batches: 0.5550
train variable_Acc: 0.9963
val total loss: 0.5600
val mean_loss of all the batches: 0.5600
val variable_Acc: 0.9912
saving with loss of 0.5600043044465095 improved over previous 0.5602120656705813
Saving Model...

Epoch 15/89
----------
train total loss: 0.5549
train mean_loss of all the batches: 0.5549
train variable_Acc: 0.9964
val total loss: 0.5597
val mean_loss of all the batches: 0.5597
val variable_Acc: 0.9915
saving with loss of 0.5597199719981993 improved over previous 0.5600043044465095
Saving Model...

Epoch 16/89
----------
train total loss: 0.5549
train mean_loss of all the batches: 0.5549
train variable_Acc: 0.9965
val total loss: 0.5598
val mean_loss of all the batches: 0.5598
val variable_Acc: 0.9914
saving with loss of 0.5598169151733984 improved over previous 0.5597199719981993
Saving Model...

Epoch 17/89
----------
train total loss: 0.5548
train mean_loss of all the batches: 0.5548
train variable_Acc: 0.9965
val total loss: 0.5598
val mean_loss of all the batches: 0.5598
val variable_Acc: 0.9915
saving with loss of 0.5597615060425263 improved over previous 0.5598169151733984
Saving Model...

Epoch 18/89
----------
train total loss: 0.5547
train mean_loss of all the batches: 0.5548
train variable_Acc: 0.9966
val total loss: 0.5597
val mean_loss of all the batches: 0.5597
val variable_Acc: 0.9916
saving with loss of 0.5596967508372215 improved over previous 0.5597615060425263
Saving Model...

Epoch 19/89
----------
train total loss: 0.5546
train mean_loss of all the batches: 0.5546
train variable_Acc: 0.9968
val total loss: 0.5595
val mean_loss of all the batches: 0.5595
val variable_Acc: 0.9917
saving with loss of 0.5594885214739085 improved over previous 0.5596967508372215
Saving Model...

Epoch 20/89
----------
train total loss: 0.5545
train mean_loss of all the batches: 0.5545
train variable_Acc: 0.9968
val total loss: 0.5597
val mean_loss of all the batches: 0.5597
val variable_Acc: 0.9915
saving with loss of 0.5597136069010545 improved over previous 0.5594885214739085
Saving Model...

Epoch 21/89
----------
train total loss: 0.5545
train mean_loss of all the batches: 0.5545
train variable_Acc: 0.9969
val total loss: 0.5595
val mean_loss of all the batches: 0.5595
val variable_Acc: 0.9918
saving with loss of 0.5594671315019257 improved over previous 0.5597136069010545
Saving Model...

Epoch 22/89
----------
train total loss: 0.5545
train mean_loss of all the batches: 0.5545
train variable_Acc: 0.9968
val total loss: 0.5595
val mean_loss of all the batches: 0.5595
val variable_Acc: 0.9918
saving with loss of 0.5594564494833382 improved over previous 0.5594671315019257
Saving Model...

Epoch 23/89
----------
train total loss: 0.5545
train mean_loss of all the batches: 0.5545
train variable_Acc: 0.9969
val total loss: 0.5595
val mean_loss of all the batches: 0.5595
val variable_Acc: 0.9917
saving with loss of 0.5595248572036009 improved over previous 0.5594564494833382
Saving Model...

Epoch 24/89
----------
train total loss: 0.5545
train mean_loss of all the batches: 0.5545
train variable_Acc: 0.9969
val total loss: 0.5594
val mean_loss of all the batches: 0.5594
val variable_Acc: 0.9919
saving with loss of 0.5593769946349062 improved over previous 0.5595248572036009
Saving Model...

Epoch 25/89
----------
train total loss: 0.5545
train mean_loss of all the batches: 0.5545
train variable_Acc: 0.9969
val total loss: 0.5594
val mean_loss of all the batches: 0.5594
val variable_Acc: 0.9919
saving with loss of 0.5593602908094577 improved over previous 0.5593769946349062
Saving Model...

Epoch 26/89
----------
