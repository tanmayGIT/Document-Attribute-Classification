Hi, I am here dear available : 
{'resume': 'janinah', 'debug': False, 'model': 'resnet', 'taskname': 'font_size', 'batch_size': 800, 'number_of_class': 3}
The cuda is available :  True
The number of classes are 3 and the batch size is 800
Training path exists
Validation path exists
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Train_Data_Server/
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Validation_Data_Server/
The size of train dataset: 8099561
The size of validation dataset: 1344016
(tensor([[[ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         ...,
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357]],

        [[ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         ...,
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405]],

        [[ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         ...,
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119]]]), tensor([1, 0, 0]), tensor([1, 0, 0]), tensor([1, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
Let's use 2 GPUs!
model and cuda mixing done
I am inside right trainer and task name is :  font_size
Epoch 0/89
----------
train total loss: 0.7326
train mean_loss of all the batches: 0.7326
train variable_Acc: 0.8132
val total loss: 0.7533
val mean_loss of all the batches: 0.7533
val variable_Acc: 0.7909
saving with loss of 0.7532972857134154 improved over previous 0.0
Saving Model...

Epoch 1/89
----------
train total loss: 0.6832
train mean_loss of all the batches: 0.6833
train variable_Acc: 0.8645
val total loss: 0.7341
val mean_loss of all the batches: 0.7341
val variable_Acc: 0.8110
saving with loss of 0.7340844807923574 improved over previous 0.7532972857134154
Saving Model...

Epoch 2/89
----------
train total loss: 0.6682
train mean_loss of all the batches: 0.6682
train variable_Acc: 0.8800
val total loss: 0.7187
val mean_loss of all the batches: 0.7187
val variable_Acc: 0.8273
saving with loss of 0.7186615406119414 improved over previous 0.7340844807923574
Saving Model...

Epoch 3/89
----------
train total loss: 0.6591
train mean_loss of all the batches: 0.6592
train variable_Acc: 0.8894
val total loss: 0.7050
val mean_loss of all the batches: 0.7050
val variable_Acc: 0.8416
saving with loss of 0.7049521807510185 improved over previous 0.7186615406119414
Saving Model...

Epoch 4/89
----------
train total loss: 0.6530
train mean_loss of all the batches: 0.6531
train variable_Acc: 0.8957
val total loss: 0.6993
val mean_loss of all the batches: 0.6993
val variable_Acc: 0.8476
saving with loss of 0.6992959940079143 improved over previous 0.7049521807510185
Saving Model...

Epoch 5/89
----------
train total loss: 0.6483
train mean_loss of all the batches: 0.6483
train variable_Acc: 0.9006
val total loss: 0.6944
val mean_loss of all the batches: 0.6944
val variable_Acc: 0.8525
saving with loss of 0.6943541000437758 improved over previous 0.6992959940079143
Saving Model...

Epoch 6/89
----------
train total loss: 0.6447
train mean_loss of all the batches: 0.6447
train variable_Acc: 0.9043
val total loss: 0.6969
val mean_loss of all the batches: 0.6969
val variable_Acc: 0.8498
saving with loss of 0.6968876783797373 improved over previous 0.6943541000437758
Saving Model...

Epoch 7/89
----------
train total loss: 0.6418
train mean_loss of all the batches: 0.6418
train variable_Acc: 0.9073
val total loss: 0.6911
val mean_loss of all the batches: 0.6911
val variable_Acc: 0.8560
saving with loss of 0.6910515143966929 improved over previous 0.6968876783797373
Saving Model...

Epoch 8/89
----------
train total loss: 0.6393
train mean_loss of all the batches: 0.6393
train variable_Acc: 0.9098
val total loss: 0.6901
val mean_loss of all the batches: 0.6901
val variable_Acc: 0.8569
saving with loss of 0.6901041547979148 improved over previous 0.6910515143966929
Saving Model...

Epoch 9/89
----------
train total loss: 0.6251
train mean_loss of all the batches: 0.6251
train variable_Acc: 0.9247
val total loss: 0.6631
val mean_loss of all the batches: 0.6631
val variable_Acc: 0.8851
saving with loss of 0.6631214854388042 improved over previous 0.6901041547979148
Saving Model...

Epoch 10/89
----------
train total loss: 0.6227
train mean_loss of all the batches: 0.6227
train variable_Acc: 0.9272
val total loss: 0.6613
val mean_loss of all the batches: 0.6613
val variable_Acc: 0.8869
saving with loss of 0.6612589550981056 improved over previous 0.6631214854388042
Saving Model...

Epoch 11/89
----------
train total loss: 0.6216
train mean_loss of all the batches: 0.6216
train variable_Acc: 0.9284
val total loss: 0.6619
val mean_loss of all the batches: 0.6619
val variable_Acc: 0.8865
saving with loss of 0.6618559830949496 improved over previous 0.6612589550981056
Saving Model...

Epoch 12/89
----------
train total loss: 0.6209
train mean_loss of all the batches: 0.6209
train variable_Acc: 0.9291
val total loss: 0.6622
val mean_loss of all the batches: 0.6622
val variable_Acc: 0.8859
saving with loss of 0.662213380991752 improved over previous 0.6618559830949496
Saving Model...

Epoch 13/89
----------
train total loss: 0.6203
train mean_loss of all the batches: 0.6203
train variable_Acc: 0.9297
val total loss: 0.6607
val mean_loss of all the batches: 0.6607
val variable_Acc: 0.8876
saving with loss of 0.6606588216652803 improved over previous 0.662213380991752
Saving Model...

Epoch 14/89
----------
train total loss: 0.6197
train mean_loss of all the batches: 0.6197
train variable_Acc: 0.9304
val total loss: 0.6616
val mean_loss of all the batches: 0.6616
val variable_Acc: 0.8867
saving with loss of 0.6615524171663195 improved over previous 0.6606588216652803
Saving Model...

Epoch 15/89
----------
