Hi, I am here dear available : 
{'resume': 'janinah', 'debug': False, 'model': 'resnet', 'taskname': 'font_type', 'batch_size': 800, 'number_of_class': 6}
The cuda is available :  True
The number of classes are 6 and the batch size is 800
Training path exists
Validation path exists
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Train_Data_Server/
Im am inside WordImageDS single word image
Image file path is in single word image : /data/zenith/user/tmondal/Font_Data/Validation_Data_Server/
The size of train dataset: 8099561
The size of validation dataset: 1344016
(tensor([[[ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         [ 1.9235,  1.9407,  2.0092,  ...,  1.5982,  1.7523,  1.8037],
         ...,
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357],
         [ 0.3652,  0.0569, -0.7308,  ..., -1.4329, -1.5014, -1.5357]],

        [[ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         [ 2.0959,  2.1134,  2.1835,  ...,  1.7633,  1.9209,  1.9734],
         ...,
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405],
         [ 0.5028,  0.1877, -0.6176,  ..., -1.3354, -1.4055, -1.4405]],

        [[ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         [ 2.3088,  2.3263,  2.3960,  ...,  1.9777,  2.1346,  2.1868],
         ...,
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119],
         [ 0.7228,  0.4091, -0.3927,  ..., -1.1073, -1.1770, -1.2119]]]), tensor([1, 0, 0]), tensor([1, 0, 0]), tensor([1, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
Let's use 2 GPUs!
model and cuda mixing done
I am inside right trainer and task name is :  font_type
Epoch 0/89
----------
train total loss: 1.2600
train mean_loss of all the batches: 1.2600
train variable_Acc: 0.7807
val total loss: 1.2991
val mean_loss of all the batches: 1.2991
val variable_Acc: 0.7405
saving with loss of 1.2990683431070538 improved over previous 0.0
Saving Model...

Epoch 1/89
----------
train total loss: 1.2020
train mean_loss of all the batches: 1.2020
train variable_Acc: 0.8400
val total loss: 1.2617
val mean_loss of all the batches: 1.2617
val variable_Acc: 0.7789

Epoch 2/89
----------
train total loss: 1.1840
train mean_loss of all the batches: 1.1841
train variable_Acc: 0.8582
val total loss: 1.2526
val mean_loss of all the batches: 1.2526
val variable_Acc: 0.7881

Epoch 3/89
----------
train total loss: 1.1731
train mean_loss of all the batches: 1.1732
train variable_Acc: 0.8693
val total loss: 1.2388
val mean_loss of all the batches: 1.2388
val variable_Acc: 0.8021

Epoch 4/89
----------
train total loss: 1.1655
train mean_loss of all the batches: 1.1655
train variable_Acc: 0.8770
val total loss: 1.2284
val mean_loss of all the batches: 1.2284
val variable_Acc: 0.8126

Epoch 5/89
----------
train total loss: 1.1598
train mean_loss of all the batches: 1.1598
train variable_Acc: 0.8828
val total loss: 1.2308
val mean_loss of all the batches: 1.2309
val variable_Acc: 0.8102

Epoch 6/89
----------
train total loss: 1.1555
train mean_loss of all the batches: 1.1555
train variable_Acc: 0.8871
val total loss: 1.2249
val mean_loss of all the batches: 1.2249
val variable_Acc: 0.8162

Epoch 7/89
----------
train total loss: 1.1516
train mean_loss of all the batches: 1.1517
train variable_Acc: 0.8909
val total loss: 1.2191
val mean_loss of all the batches: 1.2191
val variable_Acc: 0.8222

Epoch 8/89
----------
train total loss: 1.1487
train mean_loss of all the batches: 1.1488
train variable_Acc: 0.8939
val total loss: 1.2118
val mean_loss of all the batches: 1.2118
val variable_Acc: 0.8296

Epoch 9/89
----------
train total loss: 1.1320
train mean_loss of all the batches: 1.1321
train variable_Acc: 0.9110
val total loss: 1.1924
val mean_loss of all the batches: 1.1924
val variable_Acc: 0.8493

Epoch 10/89
----------
train total loss: 1.1288
train mean_loss of all the batches: 1.1289
train variable_Acc: 0.9143
val total loss: 1.1911
val mean_loss of all the batches: 1.1911
val variable_Acc: 0.8507

Epoch 11/89
----------
train total loss: 1.1275
train mean_loss of all the batches: 1.1276
train variable_Acc: 0.9156
val total loss: 1.1902
val mean_loss of all the batches: 1.1902
val variable_Acc: 0.8516

Epoch 12/89
----------
train total loss: 1.1267
train mean_loss of all the batches: 1.1267
train variable_Acc: 0.9166
val total loss: 1.1891
val mean_loss of all the batches: 1.1891
val variable_Acc: 0.8528

Epoch 13/89
----------
train total loss: 1.1259
train mean_loss of all the batches: 1.1259
train variable_Acc: 0.9174
val total loss: 1.1881
val mean_loss of all the batches: 1.1881
val variable_Acc: 0.8538

Epoch 14/89
----------
train total loss: 1.1252
train mean_loss of all the batches: 1.1253
train variable_Acc: 0.9181
val total loss: 1.1886
val mean_loss of all the batches: 1.1886
val variable_Acc: 0.8534

Epoch 15/89
----------
train total loss: 1.1247
train mean_loss of all the batches: 1.1248
train variable_Acc: 0.9186
val total loss: 1.1870
val mean_loss of all the batches: 1.1870
val variable_Acc: 0.8550

Epoch 16/89
----------
train total loss: 1.1241
train mean_loss of all the batches: 1.1242
train variable_Acc: 0.9192
val total loss: 1.1873
val mean_loss of all the batches: 1.1874
val variable_Acc: 0.8546

Epoch 17/89
----------
train total loss: 1.1236
train mean_loss of all the batches: 1.1237
train variable_Acc: 0.9197
val total loss: 1.1865
val mean_loss of all the batches: 1.1865
val variable_Acc: 0.8555

Epoch 18/89
----------
train total loss: 1.1231
train mean_loss of all the batches: 1.1232
train variable_Acc: 0.9202
val total loss: 1.1863
val mean_loss of all the batches: 1.1863
val variable_Acc: 0.8556

Epoch 19/89
----------
train total loss: 1.1215
train mean_loss of all the batches: 1.1215
train variable_Acc: 0.9219
val total loss: 1.1846
val mean_loss of all the batches: 1.1847
val variable_Acc: 0.8573

Epoch 20/89
----------
train total loss: 1.1210
train mean_loss of all the batches: 1.1211
train variable_Acc: 0.9224
val total loss: 1.1849
val mean_loss of all the batches: 1.1849
val variable_Acc: 0.8571

Epoch 21/89
----------
train total loss: 1.1209
train mean_loss of all the batches: 1.1209
train variable_Acc: 0.9226
val total loss: 1.1847
val mean_loss of all the batches: 1.1847
val variable_Acc: 0.8572

Epoch 22/89
----------
train total loss: 1.1208
train mean_loss of all the batches: 1.1208
train variable_Acc: 0.9227
val total loss: 1.1848
val mean_loss of all the batches: 1.1848
val variable_Acc: 0.8573

Epoch 23/89
----------
train total loss: 1.1207
train mean_loss of all the batches: 1.1207
train variable_Acc: 0.9228
val total loss: 1.1846
val mean_loss of all the batches: 1.1846
val variable_Acc: 0.8574

Epoch 24/89
----------
